# Scraper Aut√≥nomo de Veh√≠culos y Autopartes

Un scraper inteligente que explora la web de forma aut√≥noma, identifica p√°ginas relevantes sobre veh√≠culos (a√±o, marca, modelo) y autopartes, y extrae datos estructurados guard√°ndolos en una base de datos SQLite.

## üöÄ Caracter√≠sticas

- **Exploraci√≥n Aut√≥noma**: Navega por la web sin necesidad de URLs espec√≠ficas
- **Detecci√≥n Inteligente**: Identifica autom√°ticamente contenido relevante sobre veh√≠culos y autopartes
- **Extracci√≥n de Datos**: Extrae informaci√≥n estructurada (a√±o, marca, modelo, condici√≥n, descripci√≥n)
- **Base de Datos SQLite**: Almacena todos los datos en una base de datos local
- **Navegaci√≥n Inteligente**: Sigue enlaces prometedores bas√°ndose en relevancia
- **Manejo de Errores Robusto**: üõ°Ô∏è Sistema completo de gesti√≥n de URLs inv√°lidas
  - Registra p√°ginas bloqueadas, no responsivas o con errores
  - Sistema de reintentos autom√°ticos (hasta 3 intentos)
  - Clasificaci√≥n de errores por tipo
- **Reanudaci√≥n Autom√°tica**: üîÑ Puede continuar desde donde se qued√≥
  - Detecta URLs pendientes de procesar
  - No pierde progreso si se interrumpe
  - Evita procesar URLs duplicadas
- **Soporte para P√°ginas Din√°micas**: ‚úÖ Compatible con React, Vue, Angular y otras SPAs (Single Page Applications)
  - Espera autom√°tica a que el JavaScript se ejecute
  - Manejo de contenido lazy-loaded
  - Scroll autom√°tico para cargar contenido infinito
  - Detecci√≥n de elementos renderizados din√°micamente
- **Protecci√≥n Anti-Bot**: üõ°Ô∏è M√∫ltiples t√©cnicas para evadir detecci√≥n
  - Puppeteer Stealth Plugin para ocultar automatizaci√≥n
  - Rotaci√≥n de User Agents realistas
  - Headers HTTP realistas y variables
  - Simulaci√≥n de comportamiento humano (movimientos de mouse, scroll)
  - Delays aleatorios entre requests
  - Detecci√≥n y manejo de CAPTCHAs/bloqueos
  - Fingerprinting mejorado (WebGL, plugins, etc.)

## üìã Requisitos

- Node.js 18 o superior
- npm o yarn

## üîß Instalaci√≥n

1. Instalar dependencias:

```bash
npm install
```

2. Ejecutar el scraper:

```bash
npm start
```

## üìÅ Estructura del Proyecto

```
scrapperVehicle/
‚îú‚îÄ‚îÄ index.js              # Punto de entrada principal
‚îú‚îÄ‚îÄ crawler.js            # Crawler aut√≥nomo
‚îú‚îÄ‚îÄ antiBot.js            # M√≥dulo de protecci√≥n anti-bot
‚îú‚îÄ‚îÄ dataExtractor.js      # Extractor de datos
‚îú‚îÄ‚îÄ relevanceDetector.js  # Detector de relevancia
‚îú‚îÄ‚îÄ database.js           # Gesti√≥n de base de datos SQLite
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ README.md
```

## üóÑÔ∏è Base de Datos

El scraper crea una base de datos SQLite (`vehicles.db`) con las siguientes tablas:

### `visited_urls`
Almacena todas las URLs visitadas con su score de relevancia.

### `vehicles`
Almacena informaci√≥n de veh√≠culos extra√≠dos:
- A√±o
- Marca
- Modelo
- Condici√≥n (nuevo/usado/seminuevo)
- Descripci√≥n

**Nota**: Los veh√≠culos son √∫nicos por la combinaci√≥n de marca + modelo + a√±o. No se extraen precio ni kilometraje.

### `parts`
Almacena informaci√≥n de autopartes:
- Nombre de la parte
- N√∫mero de parte
- Marca
- Veh√≠culo compatible
- Descripci√≥n

### `invalid_urls`
Almacena URLs que no pudieron ser procesadas:
- URL
- Tipo de error (`blocked`, `timeout`, `protocol_error`, `network_error`, `not_found`, `unknown_error`)
- Mensaje de error
- Fecha de fallo
- Contador de reintentos

**Nota**: Las URLs se marcan como inv√°lidas cuando:
- La p√°gina est√° bloqueada por protecci√≥n anti-bot
- Hay timeouts de navegaci√≥n o protocolo
- Errores de red o conexi√≥n
- P√°ginas no encontradas (404)
- Otros errores no clasificados

El sistema permite hasta 3 reintentos autom√°ticos antes de descartar una URL definitivamente.

## ‚öôÔ∏è Configuraci√≥n

Puedes modificar los par√°metros en `index.js`:

```javascript
crawler.maxPages = 50;  // N√∫mero m√°ximo de p√°ginas a visitar
crawler.maxDepth = 2;   // Profundidad m√°xima de navegaci√≥n
crawler.delay = 2000;   // Delay entre requests (ms)
```

## üîç C√≥mo Funciona

1. **Inicio**: El scraper comienza con URLs semilla (sitios conocidos de veh√≠culos)
   - Si hay URLs pendientes de ejecuciones anteriores, las carga autom√°ticamente
2. **Navegaci√≥n**: Visita cada p√°gina y analiza su contenido
   - Verifica si la URL ya fue procesada exitosamente
   - Evita procesar URLs marcadas como inv√°lidas (con 3+ intentos fallidos)
3. **Detecci√≥n**: Calcula un score de relevancia basado en palabras clave
4. **Extracci√≥n**: Si la p√°gina es relevante, extrae datos estructurados
5. **Exploraci√≥n**: Agrega enlaces prometedores a la cola para visitar
6. **Almacenamiento**: Guarda todos los datos en SQLite
7. **Manejo de Errores**: Si una p√°gina falla:
   - Se marca como inv√°lida con el tipo de error correspondiente
   - Se permite reintento autom√°tico (hasta 3 veces)
   - El proceso contin√∫a con la siguiente URL sin interrumpirse

## üìä Consultar Datos

Puedes consultar la base de datos usando cualquier cliente SQLite:

```bash
sqlite3 vehicles.db

# Ejemplos de consultas:
SELECT * FROM vehicles LIMIT 10;
SELECT brand, COUNT(*) FROM vehicles GROUP BY brand;
SELECT * FROM parts WHERE brand = 'Toyota';

# Consultar URLs inv√°lidas:
SELECT * FROM invalid_urls ORDER BY failed_at DESC;
SELECT error_type, COUNT(*) FROM invalid_urls GROUP BY error_type;
SELECT * FROM invalid_urls WHERE retry_count >= 3;  # URLs descartadas definitivamente
```

## üõ°Ô∏è Protecci√≥n Anti-Bot

El scraper incluye m√∫ltiples t√©cnicas para evadir detecci√≥n:

### T√©cnicas Implementadas

1. **Puppeteer Stealth Plugin**: Oculta indicadores de automatizaci√≥n
   - Elimina `navigator.webdriver`
   - Modifica propiedades del navegador
   - Oculta caracter√≠sticas de headless

2. **Rotaci√≥n de User Agents**: Usa User Agents reales y actualizados
   - Chrome, Firefox, Safari, Edge
   - Diferentes sistemas operativos
   - Versiones actualizadas

3. **Headers HTTP Realistas**: Headers que simulan navegadores reales
   - Accept-Language apropiado
   - Sec-Fetch-* headers
   - Referer cuando corresponde

4. **Simulaci√≥n de Comportamiento Humano**:
   - Movimientos aleatorios del mouse
   - Scroll suave y progresivo
   - Clicks ocasionales en elementos
   - Delays aleatorios entre acciones

5. **Detecci√≥n de Bloqueos**:
   - Detecta CAPTCHAs autom√°ticamente
   - Identifica p√°ginas de bloqueo (Cloudflare, etc.)
   - Intenta evadir bloqueos con recargas

6. **Fingerprinting Mejorado**:
   - WebGL spoofing
   - Plugins simulados
   - Permisos del navegador

### Limitaciones

‚ö†Ô∏è **Importante**: Estas t√©cnicas mejoran las posibilidades de evadir detecci√≥n, pero:
- No garantizan 100% de √©xito contra todas las protecciones
- Sitios con protecciones avanzadas (Cloudflare Enterprise, etc.) pueden seguir bloqueando
- Algunos sitios requieren soluciones m√°s avanzadas (resoluci√≥n de CAPTCHAs, proxies rotativos, etc.)

## ‚ö†Ô∏è Consideraciones

- **Respeto a robots.txt**: El scraper no verifica robots.txt autom√°ticamente. √ösalo responsablemente.
- **Rate Limiting**: Incluye delays aleatorios entre requests para no sobrecargar servidores
- **Legalidad**: Aseg√∫rate de cumplir con los t√©rminos de servicio de los sitios que visites
- **Rendimiento**: El proceso puede ser lento debido a delays y simulaci√≥n de comportamiento humano
- **√âtica**: Usa el scraper de forma responsable y respeta las pol√≠ticas de los sitios web

## üõ†Ô∏è Desarrollo

Para desarrollo con auto-reload:

```bash
npm run dev
```

## üìù Notas T√©cnicas

- **Puppeteer Extra**: Usa `puppeteer-extra` con plugins stealth para mejor evasi√≥n
- **P√°ginas Din√°micas**: Soporta completamente React, Vue, Angular y otras SPAs:
  - Espera a que el DOM est√© completamente renderizado
  - Detecta elementos comunes de frameworks (React root, Vue app, Angular ng-app)
  - Maneja contenido as√≠ncrono y lazy-loading
  - Hace scroll autom√°tico para cargar contenido infinito
- **Detecci√≥n de Relevancia**: Se basa en an√°lisis de texto y patrones
- **Limpieza de Datos**: Los datos extra√≠dos pueden requerir limpieza manual dependiendo de la fuente
- **Delays Aleatorios**: Los tiempos de espera son aleatorios para simular comportamiento humano real
- **Manejo de Errores**: Sistema robusto que:
  - Clasifica errores por tipo (blocked, timeout, protocol_error, network_error, not_found, unknown_error)
  - Permite reintentos autom√°ticos (hasta 3 intentos)
  - No interrumpe el proceso cuando una p√°gina falla
  - Registra todos los errores para an√°lisis posterior
- **Reanudaci√≥n**: El scraper puede continuar desde donde se qued√≥:
  - Detecta URLs visitadas pero sin datos extra√≠dos
  - Carga autom√°ticamente URLs pendientes al reiniciar
  - Solo marca URLs como visitadas cuando se procesan exitosamente
- **Protocol Timeout**: Configurado a 300 segundos para manejar p√°ginas complejas
- **Unicidad de Datos**: Los veh√≠culos son √∫nicos por combinaci√≥n marca+modelo+a√±o, evitando duplicados

## üìÑ Licencia

MIT

